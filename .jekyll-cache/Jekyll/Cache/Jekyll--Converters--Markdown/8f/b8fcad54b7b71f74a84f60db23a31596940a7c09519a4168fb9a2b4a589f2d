I"
—<!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">MICRO</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2022genpip" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GenPIP: In-Memory Acceleration of Genome Analysis via Tight Integration of Basecalling and Read Mapping</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Mohammed Alser,&nbsp;Mohammad Sadrosadati,&nbsp;Can Firtina,&nbsp;Akanksha Baranwal,&nbsp;Damla Senol Cali,&nbsp;Aditya Manglik,&nbsp;Nour Almadhoun Alserr,&nbsp;and&nbsp;Onur Mutlu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>MICRO</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/genpip.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Nanopore sequencing is a widely-used high-throughput genome sequencing technology that can sequence long fragments of a genome into raw electrical signals at low cost. Nanopore sequencing requires two computationally-costly processing steps for accurate downstream genome analysis. The first step, basecalling, translates the raw electrical signals into nucleotide bases (i.e., A, C, G, T). The second step, read mapping, finds the correct location of a read in a reference genome. In existing genome analysis pipelines, basecalling and read mapping are executed separately. We observe in this work that such separate execution of the two most time-consuming steps inherently leads to (1) significant data movement and (2) redundant computations on the data, slowing down the genome analysis pipeline. This paper proposes GenPIP, an in-memory genome analysis accelerator that tightly integrates basecalling and read mapping. GenPIP improves the performance of the genome analysis pipeline with two key mechanisms: (1) in-memory fine-grained collaborative execution of the major genome analysis steps in parallel; (2) a new technique for early-rejection of low-quality and unmapped reads to timely stop the execution of genome analysis for such reads, reducing inefficient computation. Our experiments show that, for the execution of the genome analysis pipeline, GenPIP provides 41.6Ã— (8.4Ã—) speedup and 32.8Ã— (20.8Ã—) energy savings with negligible accuracy loss compared to the state-of-the-art software genome analysis tools executed on a state-of-the-art CPU (GPU). Compared to a design that combines state-of-the-art in-memory basecalling and read mapping accelerators, GenPIP provides 1.39Ã— speedup and 1.37Ã— energy savings.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mao2022genpip</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GenPIP: In-Memory Acceleration of Genome Analysis via Tight Integration of Basecalling and Read Mapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Alser, Mohammed and Sadrosadati, Mohammad and Firtina, Can and Baranwal, Akanksha and Cali, Damla Senol and Manglik, Aditya and Alserr, Nour Almadhoun and Mutlu, Onur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{MICRO}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CSBJ</abbr></div>

        <!-- Entry bib key -->
        <div id="alser2022molecules" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">From molecules to genomic variations: Accelerating genome analysis via intelligent algorithms and architectures</div>
          <!-- Author -->
          <div class="author">
          

          Mohammed Alser,&nbsp;Joel Lindegger,&nbsp;Can Firtina,&nbsp;Nour Almadhoun,&nbsp;<em>Haiyu Mao</em>,&nbsp;Gagandeep Singh,&nbsp;Juan Gomez-Luna,&nbsp;and&nbsp;Onur Mutlu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>Computational and Structural Biotechnology Journal</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/genanalysis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We now need more than ever to make genome analysis more intelligent. We need to read, analyze, and interpret our genomes not only quickly, but also accurately and efficiently enough to scale the analysis to population level. There currently exist major computational bottlenecks and inefficiencies throughout the entire genome analysis pipeline, because state-of-the-art genome sequencing technologies are still not able to read a genome in its entirety. We describe the ongoing journey in significantly improving the performance, accuracy, and efficiency of genome analysis using intelligent algorithms and hardware architectures. We explain state-of-the-art algorithmic methods and hardware-based acceleration approaches for each step of the genome analysis pipeline and provide experimental evaluations. Algorithmic approaches exploit the structure of the genome as well as the structure of the underlying hardware. Hardware-based acceleration approaches exploit specialized microarchitectures or various execution paradigms (e.g., processing inside or near memory) along with algorithmic changes, leading to new hardware/software co-designed systems. We conclude with a foreshadowing of future challenges, benefits, and research directions triggered by the development of both very low cost yet highly error prone new sequencing technologies and specialized hardware chips for genomics. We hope that these efforts and the challenges we discuss provide a foundation for future work in making genome analysis more intelligent.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">alser2022molecules</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From molecules to genomic variations: Accelerating genome analysis via intelligent algorithms and architectures}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alser, Mohammed and Lindegger, Joel and Firtina, Can and Almadhoun, Nour and Mao, Haiyu and Singh, Gagandeep and Gomez-Luna, Juan and Mutlu, Onur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computational and Structural Biotechnology Journal}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ASPLOS</abbr></div>

        <!-- Entry bib key -->
        <div id="mansouri2022genstore" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GenStore: a high-performance in-storage processing system for genome sequence analysis</div>
          <!-- Author -->
          <div class="author">
          

          Nika Mansouri Ghiasi,&nbsp;Jisung Park,&nbsp;Harun Mustafa,&nbsp;Jeremie Kim,&nbsp;Ataberk Olgun,&nbsp;Arvid Gollwitzer,&nbsp;Damla Senol Cali,&nbsp;Can Firtina,&nbsp;<em>Haiyu Mao</em>,&nbsp;Nour Almadhoun Alserr,&nbsp;and&nbsp; others</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/genstore.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems. We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1) different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2) different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05Ã— (1.52-3.32Ã—) for read sets with high similar- ity to the reference genome and 1.45-33.63Ã— (2.70-19.2Ã—) for read sets with low similarity to the reference genome.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mansouri2022genstore</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GenStore: a high-performance in-storage processing system for genome sequence analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">SSI</abbr></div>

        <!-- Entry bib key -->
        <div id="haiyu2021development" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Development of processing-in-memory</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Jiwu Shu,&nbsp;Fei Li,&nbsp;and&nbsp;Zhe Liu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>SCIENTIA SINICA Informationis</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/pimsurvey.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the explosive increase of processed data, data transmission through the bus between CPU and the main memory has become a bottleneck in the traditional von Neumann architecture. On top of this, popular data-intensive workloads, such as neural networks and graph computing applications, have poor data locality, which results in a substantial increase of the cache miss rate. Processing such popular data-intensive workloads hinders the entire system since the data transmission causes long latency and high energy consumption. Processing-in-memory greatly reduces this data transmission by equipping the main memory with computation ability, alleviating the problems of poor performance and high energy consumption caused by a large amount of data and a poor data locality. Processing-in-memory consists of two different approaches. One method involves integrating computation resources into the main memory with high-bandwidth interconnects (i.e., near data computing). The other method consists of employing memory arrays to compute directly (i.e., computing-in-memory). These two approaches have their own advantages and disadvantages, as well as suitable scenarios. In this survey, the birth and development of processing-in-memory is firstly introduced and discussed. Its techniques, ranging from hardware to microarchitecture, are then presented. Furthermore, the challenges faced by processing-in-memory are analyzed. Finally, the opportunities that processing-in-memory offers for popular applications are discussed.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">haiyu2021development</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development of processing-in-memory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Shu, Jiwu and Li, Fei and Liu, Zhe}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SCIENTIA SINICA Informationis}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Science China Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TC</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2020lrgan" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">LrGAN: A Compact and Energy Efficient PIM-Based Architecture for GAN Training</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Jiwu Shu,&nbsp;Mingcong Song,&nbsp;and&nbsp;Tao Li</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>IEEE Transactions on Computers</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/LrGAN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As a powerful unsupervised learning method, Generative Adversarial Network (GAN) plays an essential role in many domains. However, training a GAN imposes four more challenges: (1) intensive communication caused by complex train phases of GAN, (2) much more ineffectual computations caused by peculiar convolutions, (3) more frequent off-chip memory accesses for exchanging intermediate data between the generator and the discriminator and (4) high energy consumption of unnecessary fine-grained MLC programming. In this paper, we propose LrGAN, a PIM-based GAN accelerator, to address the challenges of training GAN. We first propose a zero-free data reshaping scheme for ReRAM-based PIM, which removes the zero-related computations. We then propose a 3D-connected PIM, which can reconfigure connections inside PIM dynamically according to dataflows of propagation and updating. After that, we propose an approximate weight update algorithm to avoid unnecessary fine-grain MLC programming. Finally, we propose LrGAN based on these three techniques, providing different levels of accelerating GAN for programmers. Experiments show that LrGAN achieves 47.2Ã—, 21.42Ã—, and 7.46Ã— speedup over FPGA-based GAN accelerator, GPU platform, and ReRAM-based neural network accelerator respectively. Besides, LrGAN achieves 13.65Ã—, 10.75Ã—, and 1.34Ã— energy saving on average over GPU platform, PRIME, and FPGA-based GAN accelerator, respectively.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mao2020lrgan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LrGAN: A Compact and Energy Efficient PIM-Based Architecture for GAN Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Shu, Jiwu and Song, Mingcong and Li, Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Computers}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TOS</abbr></div>

        <!-- Entry bib key -->
        <div id="yang2020shieldnvm" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ShieldNVM: An efficient and fast recoverable system for secure non-volatile memory</div>
          <!-- Author -->
          <div class="author">
          

          Fan Yang,&nbsp;Youmin Chen,&nbsp;<em>Haiyu Mao</em>,&nbsp;Youyou Lu,&nbsp;and&nbsp;Jiwu Shu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>ACM Transactions on Storage</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/Libra.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Data encryption and authentication are essential for secure non-volatile memory (NVM). However, the in- troduced security metadata needs to be atomically written back to NVM along with data, so as to provide crash consistency, which unfortunately incurs high overhead. To support fine-grained data protection and fast recovery for a secure NVM system without compromising the performance, we propose ShieldNVM. It first proposes an epoch-based mechanism to aggressively cache the security metadata in the metadata cache while retaining the consistency of them in NVM. Deferred spreading is also introduced to reduce the calcu- lating overhead for data authentication. Leveraging the ability of data hash message authentication codes, we can always recover the consistent but old security metadata to its newest version. By recording a limited number of dirty addresses of the security metadata, ShieldNVM achieves fast recovering the secure NVM sys- tem after crashes. Compared to Osiris, a state-of-the-art secure NVM, ShieldNVM reduces system runtime by 39.1% and hash message authentication code computation overhead by 80.5% on average over NVM work- loads. When system crashes happen, ShieldNVMâ€™s recovery time is orders of magnitude faster than Osiris. In addition, ShieldNVM also recovers faster than AGIT, which is the Osiris-based state-of-the-art mechanism addressing the recovery time of the secure NVM system. Once the recovery process fails, instead of dropping all data due to malicious attacks, ShieldNVM is able to detect and locate the area of the tampered data with the help of the tracked addresses.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2020shieldnvm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShieldNVM: An efficient and fast recoverable system for secure non-volatile memory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Fan and Chen, Youmin and Mao, Haiyu and Lu, Youyou and Shu, Jiwu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Storage}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM New York, NY, USA}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">DAC</abbr></div>

        <!-- Entry bib key -->
        <div id="yang2019no" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">No compromises: Secure NVM with crash consistency, write-efficiency and high-performance</div>
          <!-- Author -->
          <div class="author">
          

          Fan Yang,&nbsp;Youyou Lu,&nbsp;Youmin Chen,&nbsp;<em>Haiyu Mao</em>,&nbsp;and&nbsp;Jiwu Shu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In 2019 56th ACM/IEEE Design Automation Conference</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/SecureNVM.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Data encryption and authentication are essential for secure NVM. However, the introduced security metadata needs to be atomically written back to NVM along with data, so as to provide crash consistency, which unfortunately incurs high overhead. To support fine-grained data protection without compromising the performance, we propose cc-NVM. It firstly proposes an epoch-based mechanism to aggressively cache the security metadata in CPU cache while retaining the consistency of them in NVM. Deferred spread- ing is also introduced to reduce the calculating overhead for data authentication. Leveraging the hidden ability of data HMACs, we can always recover the consistent but old security metadata to its newest version. Compared to Osiris, a state-of-the-art secure NVM, cc-NVM improves performance by 20.4% on average. When the system crashes, instead of dropping all the data due to malicious attacks, cc-NVM is able to detect and locate the exact tampered data while only incurring extra write traffic by 29.6% on average.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2019no</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{No compromises: Secure NVM with crash consistency, write-efficiency and high-performance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Fan and Lu, Youyou and Chen, Youmin and Mao, Haiyu and Shu, Jiwu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 56th ACM/IEEE Design Automation Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">MICRO</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2018lergan" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Lergan: A zero-free, low data movement and pim-based gan architecture</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Mingcong Song,&nbsp;Tao Li,&nbsp;Yuting Dai,&nbsp;and&nbsp;Jiwu Shu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/legan.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://www.youtube.com/watch?v=dmsGaoJKbAU" class="btn btn-sm z-depth-0" role="button">Lightning talk</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As a powerful unsupervised learning method, Generative Adversarial Network (GAN) plays an important role in many domains such as video prediction and autonomous driving. It is one of the ten breakthrough technologies in 2018 reported in MIT Technology Review. However, training a GAN imposes three more challenges: (1) intensive communication caused by complex train phases of GAN, (2) much more ineffectual computations caused by special convolutions, and (3) more frequent off-chip memory accesses for exchanging inter-mediate data between the generator and the discriminator. In this paper, we propose LerGAN, a PIM-based GAN accelerator to address the challenges of training GAN. We first propose a zero-free data reshaping scheme for ReRAM-based PIM, which removes the zero-related computations. We then propose a 3D-connected PIM, which can reconfigure connections inside PIM dynamically according to dataflows of propagation and updating. Our proposed techniques reduce data movement to a great extent, avoiding I/O to become a bottleneck of training GANs. Finally, we propose LerGAN based on these two techniques, providing different levels of accelerating GAN for programmers. Experiments shows that LerGAN achieves 47.2Ã—, 21.42Ã— and 7.46Ã— speedup over FPGA-based GAN accelerator, GPU platform, and ReRAM-based neural network accelerator respectively. Moreover, LerGAN achieves 9.75Ã—, 7.68Ã— energy saving on average over GPU platform, ReRAM-based neural network accelerator respectively, and has 1.04Ã— energy consuming over FPGA-based GAN accelerator.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mao2018lergan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lergan: A zero-free, low data movement and pim-based gan architecture}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Song, Mingcong and Li, Tao and Dai, Yuting and Shu, Jiwu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2018 51st Annual IEEE/ACM International Symposium on Microarchitecture}</span><span class="p">,</span>
  <span class="na">lightningtalk</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=dmsGaoJKbAU}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">DATE</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2017protect" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Protect non-volatile memory from wear-out attack based on timing difference of row buffer hit/miss</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Xian Zhang,&nbsp;Guangyu Sun,&nbsp;and&nbsp;Jiwu Shu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Design, Automation &amp; Test in Europe Conference &amp; Exhibition</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/wearout.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Non-volatile Memories (NVMs), such as PCM and ReRAM, have been widely proposed for future main memory design because of their low standby power, high storage density, fast access speed. However, these NVMs suffer from the write endurance problem. In order to prevent a malicious program from wearing out NVMs deliberately, researchers have proposed various wear-leveling methods, which remap logical addresses to physical addresses randomly and dynamically. However, we discover that side channel leakage based on NVM row buffer hit information can reveal details of address remappings. Consequently, it can be leveraged to side-step the wear-leveling. Our simulation shows that the proposed attack method in this paper can wear out a NVM within 137 seconds, even with the protection of state-of-the-art wear-leveling schemes. To counteract this attack, we further introduce an effective countermeasure named Intra-Row Swap (IRS) to hide the wear-leveling details. The basic idea is to enable an additional intra-row block swap when a new logical address is remapped to the memory row. Experiments demonstrate that IRS can secure NVMs with negligible timing/energy overhead, compared with previous works.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mao2017protect</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Protect non-volatile memory from wear-out attack based on timing difference of row buffer hit/miss}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Zhang, Xian and Sun, Guangyu and Shu, Jiwu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NVMSA</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2015exploring" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Exploring data placement in racetrack memory based scratchpad memory</div>
          <!-- Author -->
          <div class="author">
          

          <em>Haiyu Mao</em>,&nbsp;Chao Zhang,&nbsp;Guangyu Sun,&nbsp;and&nbsp;Jiwu Shu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In IEEE Non-Volatile Memory System and Applications Symposium</em> 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/racetrack.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Scratchpad Memory (SPM) has been widely adopted in various computing systems to improve performance of data access. Recently, non-volatile memory technologies (NVMs) have been employed for SPM design to improve its capacity and reduce its energy consumption. In this paper, we explore data allocation in SPM based on racetrack memory (RM), which is an emerging NVM with ultra-high storage density and fast access speed. Since a shift operation is needed to access data in RM, data allocation has an impact on performance of RM based SPM. Several allocation methods have been discussed and compared in this work. Especially, we addressed how to leverage genetic algorithm to achieve near-optimal data allocation.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mao2015exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring data placement in racetrack memory based scratchpad memory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Haiyu and Zhang, Chao and Sun, Guangyu and Shu, Jiwu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Non-Volatile Memory System and Applications Symposium}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


</div>
:ET